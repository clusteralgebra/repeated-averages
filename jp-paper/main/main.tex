\documentclass[12pt]{article}
\usepackage{../../style}
\usepackage[margin=0.75in]{geometry}
\title{Random Iterated Averages}
\author{Alan Yan \\ Project Advisor: Professor Allan Sly}
\begin{document}

\maketitle
\tableofcontents

\newpage 

\section{Introduction}

In \cite{chatterjee2021phase}, Chatterjee, Diaconis, Sly, and Zhang are interested in the following Markov chain. 
\begin{defn}[Markov Chain I]
	We have a Markov chain $x_0, x_1, \ldots$ where $x_0$ is some initial vector in $\RR^n$. Notation-wise, let $x_k = (x_{k, 1}, \ldots, x_{k,n})$. The Markov chain progresses as follows: given $x_k$ pick two distinct coordinates $I$ and $J$ uniformly at random, and replace $x_{k, I}$ and $x_{k, J}$ by their average $\frac{x_{k, I} + x_{k, J}}{2}$.
\end{defn}
We want to study the rate of convergence of this Markov chain to its average $(\ol{x}_0, \ldots, \ol{x}_0)$ where
\[
	\ol{x_0} = \frac{x_{0, 1} + \ldots + x_{0, n}}{n}.	
\]
Without loss of generality, we may assume $\bar{x_0} = 0$ unless otherwise specified. We can measure the distance of our Markov chain to the average in many ways. The functions $T(k)$ and $S(k)$ defined below are the $L^1$ and $L^2$ measurements. 
\begin{align*}
	T(k) & := \sum_{i = 1}^n |x_{k, i} - \ol{x}_0| \\
	S(k) & := \sum_{i = 1}^n |x_{k, i} - \ol{x}_0|^2.
\end{align*}

For the $L^2$ norm measurement, an exact value is given in \cite{chatterjee2021phase}.

\begin{prop}[Proposition 2.1 in \cite{chatterjee2021phase}]
	For any $k \geq 0$, we have 
	\[
		\EE (S(k+1) | \mcF_k) = \left ( 1 - \frac{1}{n-1} \right ) S(k).
	\]	
\end{prop}

This formula gives us an exact formla for the decay of the $L^2$ distance of our Markov chain to the average. The main result in the paper was the behavior of the $L^1$ decay. 

\begin{thm}[Theorem 1.2 in \cite{chatterjee2021phase}] \label{main-theorem}
	Let $\Phi : \RR \to [0, 1]$ be the cumulative distribution function of the standard normal distribution $N(0, 1)$. For $x_0 = (1, 0, \ldots, 0)$, and any $a \in \RR$, as $n \to \infty$ we have 
	\[
		T(\lfloor n(\log_2(n) + a \sqrt{\log_2(n)}) \rfloor) \xrightarrow{\PP} 2 \Phi(-a)
	\]
	where the convergence is in probability. 
\end{thm}

Clearly, Theorem~\ref{main-theorem} implies the following result. 

\begin{cor}[Theorem 1.1 in \cite{chatterjee2021phase}]
	For $x_0 = (1, 0, \ldots, 0)$ as $n \to \infty$ we have $T(\theta n \log n) \xrightarrow{\PP} 2$ for $\theta < 1/(2 \log 2)$ and $T(\theta n \log n) \xrightarrow{\PP} 0$ for $\theta > 1/(2 \log 2)$. 
\end{cor}

In the proof of Theorem~\ref{main-theorem}, to avoid issues of dependence in the discrete time setting, Markov chain I is approximated with a Poissonized continuous-time version of the same Markov chain. 

\begin{defn}[Markov Chain II]
	Denote our Markov chain by $\{x_t\}_{t \geq 0}$. Give each pair of coordinates an independent Poisson clock with rate $\binom{n}{2}^{-1}$. When a clock rings, replace the corresponding pair of coordinates with their average. 
\end{defn}

By giving each pair of coordinates a clock of rate $\binom{n}{2}^{-1}$, we make it so that on average, one averaging happens every unit of time. Thus, this Poissonized version is a suitable approximation of our original process. With this new Markov chain, we introduce the same $L^1$ and $L^2$ measurements from the average:

\begin{align*}
	T'(k) & := \sum_{i = 1}^n \abs{x_{t, i} - \ol{x}_0} \\
	S'(k) & := \sum_{i = 1}^n \abs{x_{t, i} - \ol{x}_0}^2.
\end{align*}

Similar to the discrete case, we are still able to get an exact formula for the $L^2$ measurement. 

\begin{prop}
	For any $t \geq s \geq 0$, we have that 
	\[
		\EE[S'(t) | \mcF_s ] = \exp \left ( - \frac{t-s}{n-1} \right ) S'(s).	
	\]
\end{prop}
\begin{proof}
	Let $E_k$ be the event that there are $k$ rings between times $s$ and $t$. Then 
	\begin{align*}
		\EE [ S'(t) | \mcF_s ] & = \sum_{k = 0}^\infty \EE \bparens{S'(t) | \mcF_s \text{ and $k$ rings}} \cdot \Pr[E_k] \\
		& = \sum_{k = 0}^\infty \parens{1 - \frac{1}{n-1}}^k S'(s) \cdot e^{-(t-s)} \cdot \frac{(t-s)^k}{k!} \\
		& = \exp \parens{ (t-s) \parens{1 - \frac{1}{n-1}}} \cdot e^{-(t-s)} S'(s) \\
		& = \exp \parens{- \frac{t-s}{n-1}} \cdot S'(s). 
	\end{align*}
	This suffices for the proof. 
\end{proof}

The same result $L^2$ decay result holds for the continuous-time case. 

\begin{thm}[Theorem 2.3 in \cite{chatterjee2021phase}] \label{main-theorem-continuous}
	Take $\Phi : \RR \to [0, 1]$ as the cdf of the standard normal distribution. For any $a \in \RR$, we have $T'(n (\log_2(n) +a \sqrt{\log_2(n)})/2) \pconverge 2 \Phi(-a)$ in probability as $n \to \infty$. 
\end{thm}

\begin{prop}
	Theorem~\ref{main-theorem} and Theorem~\ref{main-theorem-continuous} are equivalent. 
\end{prop}
\begin{proof}
	TODO
\end{proof}

We now consider some extensions of the problem. 

\section{Deterministic Generalized Averages}

The first generalization involves taking larger averages at each unit of time instead of simply averaging two positions. This variant is very similar to the original version, thus we won't go into all of the details. We will only do the initial computations to illustrate the similarities. 

\begin{defn}
	Let $\msM_m^n$ be the markov chain $\{x_k\}_{k \geq 0} \subset \RR^n$ where $x_0$ is some initial vector in $\RR^n$ and $x_{k+1}$ is obtained from $x_k$ by uniformly picking $m$ distinct coordinates at random and replacing them by their average. Let $\PP \msM_m^n = \{x_t\}_{t \in \RR_{\geq 0}}$ be the Poissonized version of the process where we assign independent clocks of rate $\binom{n}{m}^{-1}$ to every $m$-subset of coordinates. Whenever a clock rings, we replace the corresponding coordinates by their average. 
\end{defn}

We want to study the rate of convergence of this Markov chain to its average $(\ol{x}_0, \ldots, \ol{x}_0)$ where 
\[
	\ol{x}_0 := \frac{\langle x_0, \1 \rangle}{n}.	
\]
\begin{remark}
	Without loss of genearlity, $\ol{x}_0 = 0$ and in general the slowest rates of convergence seem to take place when all of the weight is concentrated on a single coordinate vector. Thus, we could consider initial vectors 
	\[
		x_0 = \left ( 1 - \frac{1}{n}, - \frac{1}{n}, \ldots, - \frac{1}{n} \right ).
	\]	
\end{remark}
We have two ways to measure the distance from the average:
\begin{align*}
	T(k) & := \norm{x_k - x_0}_{L^1} \\
	S(k) & := \norm{x_k - x_0}_{L^2}.
\end{align*}

\begin{prop}
	Let $\{x_k\}_{k \geq 0} = \msM_m^n$. For any $k \geq 0$, we have 
	\[
		\EE \left [ S(k+1) | \mcF_k \right ] = \left ( 1 - \frac{m-1}{n-1} \right ) S(k).	
	\]
\end{prop}

\begin{proof}
	We have 
	\begin{align*}
		\EE \left [ S(k+1) | \mcF_k \right ] & = \binom{n}{m}^{-1} \sum_{i_1 < \ldots < i_m} \left \{ S(k) - \sum_{j = 1}^m x_{k, i_j}^2 + m \left ( \frac{x_{k, i_1} + \ldots + x_{k, i_m}}{m} \right )^2 \right \} \\
		& = \binom{n}{m}^{-1} \sum_{i_1 < \ldots < i_m} \left \{ S(k) - \sum_{j = 1}^m x_{k, i_j}^2  + \frac{1}{m} \sum_{j = 1}^m x_{k, i_j}^2 + \frac{1}{m} \sum_{1 \leq u < v \leq m} x_{k, i_u} x_{k, i_v} \right \} \\
		& = \binom{n}{m}^{-1} \sum_{i_1 < \ldots < i_m} \left \{ S(k) + \frac{1-m}{m} \sum_{j = 1}^m x_{k, i_j}^2 + \frac{1}{m} \sum_{1 \leq u < v \leq m} x_{k, i_u} x_{k, i_v} \right \} \\
		& = S_1 + S_2 + S_3
	\end{align*}
	where 
	\begin{align*}
		S_1 & := \binom{n}{m}^{-1} \sum_{i_1 < \ldots < i_m} S(k) \\
		S_2 & := \binom{n}{m}^{-1} \sum_{i_1 < \ldots < i_m} \frac{1-m}{m} \sum_{j = 1}^m x_{k, i_j}^2 \\
		S_3 & = \binom{n}{m}^{-1} \sum_{i_1 < \ldots < i_m} \frac{1}{m} \sum_{1 \leq u < v \leq m} 2x_{k, i_u} x_{k, i_v}.
	\end{align*}
	For the first sum, we immediately get $S_1 = S(k)$. For the second sum, we get 
	\begin{align*}
		S_2 & = \binom{n}{m}^{-1} \cdot \frac{1-m}{m} \cdot \binom{n-1}{m-1} S(k) = \frac{1-m}{n} S(k). 
	\end{align*}
	For the last sum, we get 
	\[
		S_3 = \binom{n}{m}^{-1} \cdot \frac{1}{m} \cdot \binom{n-2}{m-2} \sum_{1 \leq u < v \leq n} 2x_{k, u} x_{k, v} = \frac{1-m}{n(n-1)} S(k)
	\]
	where we used the fact that 
	\[
		S(k) + \sum_{1 \leq u < v \leq n} 2x_{k, u} x_{k, v} = (x_{k, 1} + \ldots + x_{k, n})^2 = 0.	
	\]
	Then 
	\[
		\EE [S(k+1) | \mcF_k] = \left ( 1 + \frac{1-m}{n} + \frac{1-m}{n(n-1)} \right ) S(k) = \left ( 1 - \frac{m-1}{n-1} \right ) S(k).	
	\]
\end{proof}

\begin{cor}
	Let $\{x_t\}_{t \in \RR_{\geq 0}} = \PP \msM_m^n$. For $0 \leq s < t$, we have 
	\[
		\EE [S^*(t) | \mcF_s] = \exp \left ( \frac{(t-s)(m-1)}{n-1} \right ) S^*(s).	
	\]
\end{cor}
\begin{proof}
	We have 
	\begin{align*}
		\EE [ S^*(t) | \mcF_s ] & = \sum_{k = 0}^\infty \Pr[k \text{ rings}] \cdot \EE \left [ S^*(t) | \mcF_s \text{ and } k \text{ rings} \right ] \\
		& = \sum_{k = 0}^\infty \frac{(t-s)^k}{k!} \cdot e^{-(t-s)} \cdot \left ( 1 - \frac{m-1}{n-1} \right )^k S^*(s) \\
		& = \exp \left ( -\frac{(t-s)(m-1)}{n-1} \right ) S^*(s). 
	\end{align*}
\end{proof}

As the prevous computatoins show, the results are very similar and a similar result as Theorem~\ref{main-theorem} should hold (Maybe find the specific constant)?.

\section{Random Bounded Averages}

Now, we consider the variant where at each unit time we average a random number of elements. Let $Q$ be the random variable with distribution $(q_2, \ldots, q_N)$ where $N \geq 1$ is a fixed positive integer and $\Pr[Q = k] = q_k$ for all $k$, $2 \leq k \leq N$. We will fix this distribution throughout the section. 

\begin{defn}
	Let $\mathsf{RM} := \{x_k\}_{k \geq 0} \subset \RR^n$ be the Markov chain where $x_0$ is some initial vector and $x_{k+1}$ is obtained from $x_k$ by uniformly picking $Q_k$ distinct coordinates at random and replacing them by their average where the random variables $Q_1, Q_2, \ldots$ are independent, identically distributed with the same law as $Q$. Let $\mathsf{PRM} := \{x_k\}_{k \geq 0} \subset \RR^n$ be the Poissonized version of $\mathsf{RM}$ where we have a Poisson clock of rate $1$ and every time it rings we perform an averaging in the same way as in the discrete-time setting. 
\end{defn} 

Before we do further analysis, we first show that we can get explicit formulas for the expectations of the $L^2$ bound. The first formula is for the base version of the process $\mathsf{RM}$.

\begin{prop}
	Let $S(k)$ be the $L^2$ distance at time $k$. Then 
	\[
		\EE \left [ S(k+1) | \mcF_k \right ] = \left ( 1 - \frac{\mu - 1}{n-1} \right ) S(k). 
	\]
\end{prop}
\begin{proof}
	We have 
	\begin{align*}
		\EE[S(k+1) | \mathcal{F}_k] & = \sum_{m = 2}^N \Pr[m \text{ averaging}] \cdot \EE[S(k+1) | \mathcal{F}_k \text{ and } m \text{ averaging}] \\
		& = \sum_{m = 2}^N q_m \cdot \left ( 1 - \frac{m -1}{n-1} \right ) S(k) \\
		& = \left ( 1 + \frac{1}{n-1} - \frac{1}{n-1} \sum_{m = 2}^N mq_m \right ) S(k) \\
		& = \left ( 1 + \frac{1}{n-1} - \frac{\mu}{n-1} \right ) S(k) \\
		& = \left ( 1 - \frac{\mu - 1}{n-1} \right ) S(k). 
	\end{align*}
\end{proof}

\begin{cor}
	Let $S^*(t)$ be the $L^2$ distance at time $t$ for the Poissonized process. Then 
	\[
		\EE \left [ S^*(t) | \mcF_s \right ] = \exp \left ( - \frac{(t-s)(\mu - 1)}{n-1} \right ) S^*(s).
	\]
\end{cor}

\begin{proof}
	The proof is very similar to the proof of Corollary ???. We can compute 
	\begin{align*}
		\EE \left [S^*(t) | \mcF_s \right ] & = \sum_{k = 0}^\infty \Pr[k \text{ rings}] \cdot \EE \left [ S^*(t) | \mcF_s \text{ and } k \text{ rings} \right ] \\
		& = \sum_{k = 0}^\infty \frac{(t-s)^k}{k!} \exp (-(t-s)) \cdot \left ( 1 - \frac{\mu - 1}{n-1} \right )^k S^*(s) \\
		& = \exp \left ( - \frac{(t-s) (\mu - 1)}{n-1} \right ) S^*(s). 
	\end{align*}
\end{proof}

We can equivalently formulate $\mathsf{PRM}$ by splitting our clock of rate $1$ to $N-1$ independent Poisson clocks $C_2, \ldots, C_N$ where $C_k$ is a Poisson clock of rate $p_k$. Then our original clock is then equivalent to the amalgamation of our clocks $C_2, \ldots, C_N$ where when the $k$th clock rings we pick $k$ random positions and replace them by their average. In the original version of the problem, one key approximation step was to condition on the event that no average was performed on two non-zero elements. Let's assume in this case that the probability that we average at least two non-zero elements is negligible. (PROVE LATER?) Then, we can view our Markov chain as a type of branching process on an interval $[0, 1]$. The interval $[0, 1]$ in its initial state represents all of the weight being in one position. Averagings then are represented by an interval being split into multiple equal length intervals. The clock $C_k$ rings with rate $q_k$. When it rings, there is a probability of $k/n$ that any given interval will be split into $k$ equal parts. Equivalently, to every interval, we can attach a clock of rate $\sum_{k = 2}^N q_k \frac{k}{n} = \mu / n$ where whenever it rings, the probability we split the interval into $k$ parts is 
\[
	\frac{q_k \cdot (k/n)}{\mu /n} = \frac{k q_k}{\mu}.
\]
Let $Q'$ be the random variable with distribution $(2q_2/\mu, \ldots, Nq_N/\mu)$ where $\Pr[Q' = k] = kq_k / \mu$. So to each interval we attach a clock of rate $\mu / n$ and when it rings we split into a random number of equal parts according to the law of $Q'$. Let $I_t$ be the set of intervals at time $t$. In the proof of the original problem, we found a time where the total content of the intervals were ``close'' to $1/n$. At this time, we can then apply the $L^2$ bound which will give the correct cutoff time. In other words, we want to find a time $t(n)$ and constants $a_n, b_n$ where $1/n << a_n < b_n \sim n^{-1 + o(1)}$ such that 
\[
	\Pr \left ( \sum_{I \in I_{t(n)}} \mu(I) \cdot \1_{a_n < \mu(I) < b_n}  \geq 1 - o(1) \right ) \geq 1 - o(1).
\]
For $x \in [0, 1]$, let $I_t^x$ be the interval of the point $x$ at time $t$. Consider $\log |I_t^x|$. To this interval we have clocks $D_2, \ldots, D_N$ where $D_k$ has rate $q_k \cdot k/n$. When clock $D_k$ rings, then $\log |I_t^x|$ increases by $\log (1/k)$. This we can represent the process of $\log |I_t^x|$ as 
\[
	\log |I_t^x| = \sum_{k = 2}^N \log \left ( \frac{1}{k} \right ) \cdot \text{Poiss} \left (q_k \cdot \frac{k}{n}\right ).
\]
Thus, the distribution of $\log |I_t^x|$ for fixed $x$ is well understood. We are interested in the random variable 
\[
	X = \sum_{I \in I_{t(n)}} \mu(I) \cdot \1_{a_n < \mu(I) < b_n}.
\] 
The expected value of $X$ can be computed by exploiting the symmetry of the interval:
\[
	\EE[X] = \EE \int_0^1 \1_{|I_{t(n)}^x| \in (a_n, b_n)} \, dx = 	\int_0^1 \Pr[|I_{t(n)}^x| \in (a_n, b_n)] dx = \Pr[|I_{t(n)}^x| \in (a_n, b_n)]
\]
from Fubini's Theorem. To have any hope in producing the bound (???), we need to understand the probability $\Pr[|I_{t(n)}^x| \in (a_n, b_n)]$. Note that 
\begin{align*}
	\Pr[|I_{t(n)}^x| \in (a_n, b_n)] = \Pr[\log |I_{t(n)}^x| \in (\log a_n, \log b_n)].
\end{align*}
\subsection{Logarithmic Length of the Interval Trajectory}
Recall that for any fixed $t > 0$, we have that 
\[
	\log |I_t^x| = \sum_{k = 2}^N \log \left ( \frac{1}{k} \right ) \cdot \mathsf{Poisson} \left (\frac{q_k k}{n} \cdot t \right ).
\]

\begin{lem}
	Suppose $X_t \sim \mathsf{Poisson}(t)$ for all $t$. Then 
	\[
		\frac{X_t - t}{\sqrt{t}} \xrightarrow{d} N(0, 1)	
	\]
	where the convergence is in distribution.
\end{lem}
\begin{proof}
	Let $Y_n$ for $n \geq 1$ be iid Poisson random variables of rate $1$ and let $Z_t$ be a Poisson random variable of rate $\{t\}$. Then $X_t$ has the same distribution as 
	\[
		X_t \sim \sum_{i = 1}^{[t]} Y_i + Z_t.	
	\]
	Thus, we have that 
	\begin{align*}
		\frac{X_t - t}{\sqrt{t}} & = \frac{\sum_{i = 1}^{[t]} Y_i + Z_t-t}{\sqrt{t}} \\
		& = \frac{\sqrt{[t]}}{\sqrt{t}} \cdot \frac{\sum_{i = 1}^{[t]} Y_i - [t]}{\sqrt{[t]}} + \frac{Z_t - \{t\}}{\sqrt{t}}.
	\end{align*}
	Since $(Z_t - \{t\})/\sqrt{t} \to 0$ almost surely, the Lemma follows from the central limit theorem. 
\end{proof}
We can rewrite 
\begin{align*}
	\log |I_t^x| & = \sum_{k = 2}^N \log \left ( \frac{1}{k} \right ) \cdot \left ( P_k - \frac{q_k k}{n} \cdot t \right ) + \sum_{k = 2}^N \log \left ( \frac{1}{k} \right ) \cdot \frac{q_k k}{n} \cdot t	\\
	& = \sqrt{\frac{t}{n}} \cdot \sum_{k = 2}^N \log \left ( \frac{1}{k} \right ) \cdot \sqrt{q_k k} \cdot \left ( \frac{P_k - \frac{q_k k}{n} \cdot t}{\sqrt{\frac{q_k k}{n} t}} \right ) - \frac{t}{n} \EE[Q \log Q].
\end{align*}
Rearranging, we get that 
\[
	\frac{\log |I_t^x| + \frac{t}{n} \EE[Q \log Q]}{\sqrt{t/n}} = \sum_{k = 2}^N \log \left ( \frac{1}{k} \right ) \cdot \sqrt{q_k k} \cdot	\left ( \frac{P_k - \frac{q_k k}{n} \cdot t}{\sqrt{\frac{q_k k}{n} t}} \right ).
\]
From the Lemma, this gives 
\[
	\frac{\log |I_t^x| + \frac{t}{n} \EE[Q \log Q]}{\sqrt{t/n}} \to N \left (0, \sum_{k = 2}^N \log(k)^2 k q_k \right ) = N \left (0, \EE[Q (\log Q)^2] \right )
\]
in distribution. Define the constants
\begin{align*}
	A_n & = \sqrt{\frac{n}{t}} \cdot \left ( \log a_n + \frac{t}{n} \EE[Q \log Q] \right ) \\
	B_n & = \sqrt{\frac{n}{t}} \cdot \left ( \log b_n + \frac{t}{n} \EE[Q \log Q] \right ).
\end{align*}
Suppose $a_n = n^{-1 + a(n)}$ and $b_n = n^{-1 + b(n)}$. 

\subsection{Trying Random Stuff}

Let's say $t := t_n = \gamma n (\log n - h(n))$ where $\gamma = \EE[Q \log Q]^{-1}$ and $h(n) = o(\log n)$. Then we have 
\begin{align*}
	A_n & := \sqrt{\frac{1}{\gamma (\log n - h(n))}} \cdot \left ( (-1 + a(n)) \log n + n(\log n - h(n)) \right ) \\
	& = \frac{\log n}{\sqrt{\gamma (\log n - h(n)) }} \cdot \left ( a(n) - \frac{h(n)}{\log n} \right ) \\
	B_n & := \frac{\log n}{\sqrt{\gamma (\log n - h(n)) }} \cdot \left ( b(n) - \frac{h(n)}{\log n} \right )
\end{align*}
Then we can let, 
\begin{align*}
	a(n) & = \frac{h(n)}{2 \log n} \\
	b(n) & = \frac{2h(n)}{\log n}
\end{align*}
for example to ensure $A_n \to - \infty$ and $B_n \to \infty$. This gives us $\Pr[|I_t^x| \in (a_n, b_n)] \to 1$.

The main questions are now:

\begin{enumerate}[label = (\alph*)]
	\item Approximating the process with a new one, where we delete intervals if they get too small. Use this to prove that deleting intervals shouldn't be too bad. 

	\item 

	\item 
\end{enumerate}

\subsection{New Process}

Now that we have $a_n$, do a new process: the same but when an interval gets smaller than $a_n$ we throw it out. Moreover, if an interval gets averaged with a non-zero one, then we also throw it out. 

\begin{question}
	Let $x \in [0, 1]$. Then $\Pr[I_t^x \text{ gets averaged with non-zero pile}] \to 0$? uniformly
\end{question}

At any given point, there are at most $1/a_n$ intervals. 
\newpage 

\section{Starting Anew}

\begin{defn}
	Let $\{x_t\}$ be the Markov chain where $x_0 = (1, 0, \ldots, 0) \in \RR^n$. We have a Poisson clock of rate $1$. Everytime this clock rings, we pick $Q$ random coordinates to average. 
\end{defn}

\begin{defn}
	Let $\{\overline{x}_t\}$ be the Markov chain where $\overline{x}_0 = (1, 0 \ldots, 0) \in \RR^n$. It evolves in the same way as $\{x_t\}$ except when we average at least two positive coordinates, we throw away all of the coordinates. Moreover, when intervals become smaller than $a_n$, we also throw them away. This is a fragmentation process version of the original process where we throw out piles which become too small. 
\end{defn}

We couple the processes $\{x_t\}$ and $\{\overline{x}_t\}$ such that the same averagings happen to both of them. To the fragmentation process, there is an interval interpretation. For $x \in [0, 1]$, consider $I_t^x$, the interval containing $x$. From now on, let $t := \gamma n ( \log n - (\log n)^{3/4})$. Define 
\begin{align*}
	a_n & = n^{-1 + a(n)} \\
	b_n & = n^{-1 + b(n)} \\
	a(n) & = \frac{1}{2} (\log n)^{-1/4} \\
	b(n) & = 2 (\log n)^{-1/4}.
\end{align*}

\begin{lem}
	$\Pr[I_t^x \textit{ gets averaged with a positive pile}] \to 0$ uniformly for all $x$. 
\end{lem}
\begin{proof}
	We want to lower bound the probability that $I_t^x$ does not get averaged with a positive pile. Note that there are at most $1/a_n$ positive piles at a time. The worst possible case is if there are $1/a_n$ positive piles and $N$ terms are being averaged at every moment. Thus, the probability that $I_t^x$ does not get averaged with a positive pile is at least the probability that a Poisson process of rate $- \binom{1/a_n}{N-1} \binom{n}{N}^{-1}t$ is $0$. This gives the bounded
	\begin{align*}
		\Pr[I_t^x \text{ gets averaged with a positive pile}] & \leq 1 - \exp \left ( - \binom{1/a_n}{N-1} \binom{n}{N}^{-1} t \right ) \\
		& \leq \binom{1/a_n}{N-1} \binom{n}{N}^{-1} t \\
		& \ll n^{(N-1)(1-a(n))} \cdot n^{-N} \cdot n \log n \\
		& = n^{-(N-1)a(n)} \log n \to 0. 
	\end{align*}
	This suffices for the proof. 
\end{proof}

This implies that we can assume that no two non-zero piles get averaged. This allows us to consider the branching process as before and get that 
\[
	\EE \left [ \sum_{I \in I_t} \mu(I) \cdot \1_{a_n < \mu(I) < b_n} \right ] \to 1. 	
\]
We should (from the previous) (why?) have that 
\[
	\left | \sum_{I \in I_t} \mu(I) \cdot \1_{a_n < \mu(I) < b_n} - \sum_{i = 1}^n \overline{x}_{t, i} \cdot \1_{a_n < \overline{x}_{t, i} < b_n} \right | \to 0
\]
which also implies that 
\[
	\EE \left [ \sum_{i = 1}^n \overline{x}_{t, i} \cdot \1_{a_n < \overline{x}_{t, i} < b_n} \right ] \to 1.
\]
Then, we have 
\begin{align*}
	T(t) & = 2 \sum_{i = 1}^n \left ( x'_{t, i} - \frac{1}{n} \right )^+ \\
	& \geq 2 \sum_{i = 1}^n \left ( \bar{x}_{t, i} - \frac{1}{n} \right )^+ \\
	& \geq 2 \sum_{i = 1}^n \bar{x}_{t, i} \cdot \1_{a_n < \overline{x}_{t, i} < b_n} - \frac{2}{n} \# \{i : \bar{x}_{t, i} > a_n \} \\
	& \geq 2 \sum_{i = 1}^n \bar{x}_{t, i} \cdot \1_{a_n < \overline{x}_{t, i} < b_n} - \frac{2}{na_n}.
\end{align*}
You can check $\frac{2}{na_n} = o(1)$. Thus, we have that $T(t) \to 2$ almost surely. 

\bibliographystyle{plain}
\bibliography{ref}
\end{document}


